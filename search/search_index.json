{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Build the right thing, and build the thing right. \u00b6 This site contains a tiny sample of my notes and experience I've gathered over my professional career as a software developer and DevOps engineer. Find me on LinkedIn . It's purpose for creation was for me to learn about mkdocs and TravisCI over a weekend. Summary mkdocs: great, travisci: not-so-much. Andrew Garner Site built with MKDocs and Travic-CI","title":"Home"},{"location":"#build-the-right-thing-and-build-the-thing-right","text":"This site contains a tiny sample of my notes and experience I've gathered over my professional career as a software developer and DevOps engineer. Find me on LinkedIn . It's purpose for creation was for me to learn about mkdocs and TravisCI over a weekend. Summary mkdocs: great, travisci: not-so-much. Andrew Garner Site built with MKDocs and Travic-CI","title":"Build the right thing, and build the thing right."},{"location":"ansible/Ansible/","text":"Ansible \u00b6 Single actions \u00b6 ansible all -m setup -i 192.168.0.11, ansible all execute ansible against host group 'all' -m setup run the setup module to gather information about the hosts -i 192.168.0.11, inline host list, must be comma-separated list of hosts so add training comma for a single host. See Examples on the Ansible Setup module documentation. Local actions \u00b6 If you want to run a playbook and always do things locally, use - hosts: 127.0.0.1 in the playbook. Execute the playbook using --connection=local . This instructs ansible to execute the command locally and not to establish an SSH connection, which is useful if local loobpack is disabled eg. due to firewall rules or security groups. To run a playbook which uses - hosts: all or similar, pass an inventory with just localhost or 127.0.0.1 and connection=local ansible-playbook -i 127.0.0.1, --connection=local myplaybook.yaml Alternatively, within a playbook with tasks to be executed remotely, include a section with - hosts: 127.0.0.1 connection: local More alternatives include using the local_action module which is a shorthand syntax for the delegate_to : 127.0 . 0.1 option. Combine these with run_once : true to ensure things running locally only happen once if that's what you want (like downloading a file once to then use Ansible to push the file to multiple hosts). See Ansible documentation on Delegation This is useful to switch to running selected tasks locally in the middle of a role execution for example, where you cannot add hosts: directives to switch between local and remote execution. - hosts : all gather_facts : false tasks : - name : ensure required parameters have been set fail : msg=\"Variable '{{ item }}' is not defined\" when : item not in vars with_items : - public_key_file - host_user_id run_once : true - name : locate public key file local_action : module : stat path : \"{{ public_key_file }}\" register : keyFile run_once : true Localhost and non-SSH connections \u00b6 ansible_connection=local can be used in the inventory or on the command line to specify how Ansible will connect to the target host. In the case of local actions you can use ansible_connection=local as an inventory parameter with localhost to execute something locally even if you don't have a local loopback connection. The equivalent in a playbook is connection : local . Note localhost will be recognised by Ansible as a valid host if the /etc/hosts file has an entry for 127.0.0.1 localhost or if localhost 127.0.0.1 ansible_connection=local is specified in the inventory, otherwise use 127.0.0.1. Load variables from a file \u00b6 On the command line, use -e or --extra-vars. Set additional variables as key=value or YAML/JSON, if filename prepend the filename with @. ansible-playbook ... --extra-vars '@my-vars.yaml' Combine vars from a file and additional vars by using --extra-vars twice. ansible-playbook ... --extra-vars '@my-vars.yaml' --extra-vars 'target_env=uat generate_report=true' See documentation Roles \u00b6 Link to Roles documentation http://docs.ansible.com/ansible/2.4/playbooks_reuse.html Magic variables \u00b6 http://docs.ansible.com/ansible/latest/playbooks_variables.html#magic-variables-and-how-to-access-information-about-other-hosts The most commonly used magic variables are hostvars , groups , group_names , and inventory_hostname . groups is a list of all the groups (and hosts) in the inventory. Group and Host Variables \u00b6 See documentation for playbooks best practices . inventory/group_vars/all.yaml my_var_name: \"some value\" Use {{ hostvars [ inventory_hostname ][ 'my_var_name' ] }} to reference. Note the group_vars/all structure does not mean there is an [all] hosts group to reference, but the vars in all will get applied to all group (hosts), and can be referenced using the magic variable inventory_hostname. Note the use of single quotes for the my_var_name (which is looking up the string 'my_var_name') vs the [inventory_hostname] syntax to index the inventory_hostname in the hostvars array. Ansible Tower \u00b6 AWX is the upstream open-source version of Ansible Tower. See the AWX Project for details. You can get it from GitHub . Ansible tools \u00b6 Ansible Run Analysis (ARA) \u00b6 See the ARA article or view the code on GitHub . Ansible Google Group Ansible Source and Releases on GitHub","title":"Ansible"},{"location":"ansible/Ansible/#ansible","text":"","title":"Ansible"},{"location":"ansible/Ansible/#single-actions","text":"ansible all -m setup -i 192.168.0.11, ansible all execute ansible against host group 'all' -m setup run the setup module to gather information about the hosts -i 192.168.0.11, inline host list, must be comma-separated list of hosts so add training comma for a single host. See Examples on the Ansible Setup module documentation.","title":"Single actions"},{"location":"ansible/Ansible/#local-actions","text":"If you want to run a playbook and always do things locally, use - hosts: 127.0.0.1 in the playbook. Execute the playbook using --connection=local . This instructs ansible to execute the command locally and not to establish an SSH connection, which is useful if local loobpack is disabled eg. due to firewall rules or security groups. To run a playbook which uses - hosts: all or similar, pass an inventory with just localhost or 127.0.0.1 and connection=local ansible-playbook -i 127.0.0.1, --connection=local myplaybook.yaml Alternatively, within a playbook with tasks to be executed remotely, include a section with - hosts: 127.0.0.1 connection: local More alternatives include using the local_action module which is a shorthand syntax for the delegate_to : 127.0 . 0.1 option. Combine these with run_once : true to ensure things running locally only happen once if that's what you want (like downloading a file once to then use Ansible to push the file to multiple hosts). See Ansible documentation on Delegation This is useful to switch to running selected tasks locally in the middle of a role execution for example, where you cannot add hosts: directives to switch between local and remote execution. - hosts : all gather_facts : false tasks : - name : ensure required parameters have been set fail : msg=\"Variable '{{ item }}' is not defined\" when : item not in vars with_items : - public_key_file - host_user_id run_once : true - name : locate public key file local_action : module : stat path : \"{{ public_key_file }}\" register : keyFile run_once : true","title":"Local actions"},{"location":"ansible/Ansible/#localhost-and-non-ssh-connections","text":"ansible_connection=local can be used in the inventory or on the command line to specify how Ansible will connect to the target host. In the case of local actions you can use ansible_connection=local as an inventory parameter with localhost to execute something locally even if you don't have a local loopback connection. The equivalent in a playbook is connection : local . Note localhost will be recognised by Ansible as a valid host if the /etc/hosts file has an entry for 127.0.0.1 localhost or if localhost 127.0.0.1 ansible_connection=local is specified in the inventory, otherwise use 127.0.0.1.","title":"Localhost and non-SSH connections"},{"location":"ansible/Ansible/#load-variables-from-a-file","text":"On the command line, use -e or --extra-vars. Set additional variables as key=value or YAML/JSON, if filename prepend the filename with @. ansible-playbook ... --extra-vars '@my-vars.yaml' Combine vars from a file and additional vars by using --extra-vars twice. ansible-playbook ... --extra-vars '@my-vars.yaml' --extra-vars 'target_env=uat generate_report=true' See documentation","title":"Load variables from a file"},{"location":"ansible/Ansible/#roles","text":"Link to Roles documentation http://docs.ansible.com/ansible/2.4/playbooks_reuse.html","title":"Roles"},{"location":"ansible/Ansible/#magic-variables","text":"http://docs.ansible.com/ansible/latest/playbooks_variables.html#magic-variables-and-how-to-access-information-about-other-hosts The most commonly used magic variables are hostvars , groups , group_names , and inventory_hostname . groups is a list of all the groups (and hosts) in the inventory.","title":"Magic variables"},{"location":"ansible/Ansible/#group-and-host-variables","text":"See documentation for playbooks best practices . inventory/group_vars/all.yaml my_var_name: \"some value\" Use {{ hostvars [ inventory_hostname ][ 'my_var_name' ] }} to reference. Note the group_vars/all structure does not mean there is an [all] hosts group to reference, but the vars in all will get applied to all group (hosts), and can be referenced using the magic variable inventory_hostname. Note the use of single quotes for the my_var_name (which is looking up the string 'my_var_name') vs the [inventory_hostname] syntax to index the inventory_hostname in the hostvars array.","title":"Group and Host Variables"},{"location":"ansible/Ansible/#ansible-tower","text":"AWX is the upstream open-source version of Ansible Tower. See the AWX Project for details. You can get it from GitHub .","title":"Ansible Tower"},{"location":"ansible/Ansible/#ansible-tools","text":"","title":"Ansible tools"},{"location":"ansible/Ansible/#ansible-run-analysis-ara","text":"See the ARA article or view the code on GitHub . Ansible Google Group Ansible Source and Releases on GitHub","title":"Ansible Run Analysis (ARA)"},{"location":"ansible/filters/","text":"Filters \u00b6 Use-case \u00b6 This was developed to perform post-deployment verification against a RESTful service. The tomcat app startup script would always return before the application was ready to serve traffic and also there is a belt-and-braces check against a version endpoint to check that the url is serving the version on the application we believe should have just been deployed. The uri module handles getting what is unfortunately in this case html, then we need to extract the application version from the content. Cue Ansible Filters regex_search \u00b6 regex_search can perform the search and return the matched string. I won't share the whole html here, but note there are multiple versions in the html with no unique IDs on any of the elements. So how to extract just the application version? Although in python we can get this using a single regex and a match group, regex_search cannot return just the match group, only the whole match. < li > application version : 7.12.0-SNAPSHOT </ li > We can do this in two simple steps using regex_search, the first filter matches the project version and sets that as a fact which we can search again and this time there is only one version number in the string to be searched and extracting the version number is now easy with a second regex. - name : extract version element from response set_fact : deployed_application_version_element : \"{{ response | regex_search('application version : ([\\\\w\\\\.\\\\-]+)') }}\" - name : extract version string from element set_fact : deployed_application_version : \"{{ deployed_application_version_element | regex_search('(\\\\d+.*$)') }}\" - name : check facts debug : var : deployed_application_version Note Ansible (2.6) does not support having both facts set in a single task. The following code prodeces the error below because the first fact is not set when the second fact tries to reference it. - name : extract versions from response set_fact : deployed_application_version_element : \"{{ response | regex_search('application version : ([\\\\w\\\\.\\\\-]+)') }}\" deployed_application_version : \"{{ deployed_application_version_element | regex_search('(\\\\d+.*$)') }}\" TASK [ extract versions from response ] ***************************************************************************************** fatal: [ localhost ] : FAILED! = > { \"msg\" : \"Unexpected templating type error occurred on ({{ deployed_application_version_element | regex_search('(\\\\\\\\d+.* $ )') }}): expected string or buffer\" } The error is clearer if we drop the regex_search filter from the second fact, although as always we must read the Ansible error message carefully and fully. - name : extract versions from response set_fact : deployed_application_version_element : \"{{ response | regex_search('application version : ([\\\\w\\\\.\\\\-]+)') }}\" deployed_application_version : \"{{ deployed_application_version_element }}\" ```bash TASK [extract versions from response] ***************************************************************************************** fatal: [localhost]: FAILED! => {\"msg\": \"The task includes an option with an undefined variable. The error was: 'deployed_project_version_match' is undefined\\n\\nThe error appears to have been in '/Users/agar/code/agarthetiger/ansible/check-version.yml': line 9, column 7, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n\\n - name: extract versions from response\\n ^ here\\n\"} regex_replace \u00b6 It would be elegant to get this in one step and we can do that with regex_replace, where the replacement string can reference match groups. We need to modify the original regex slightly so the entire string is matched, in order for just the replacement match group to become the returned string. Lets break the regex down. The leading ^.* and trailing .*?$ ensure that the whole string is matched and replaced by the match group application version : matches the text in the list html element for the version we're interested in ([\\w\\.\\-]) matches any word character (letters, numbers and underscore) plus dot and hyphen and the round brackets around this expression mark it as the first (and only) match group. - name : extract application version from response set_fact : deployed_application_version : \"{{ response | regex_replace('^.*application version : ([\\\\w\\\\.\\\\-]+).*?$', '\\\\1') }}\" Online tools like pythex.org can be useful for quickly testing regular expressions. Danger Be very careful about what you paste into online tools like this. Always triple check that you are never sharing anything sensitive and if in doubt test a rexeg locally, even if it is more cumbersome. Security is never worth risking for speed.","title":"Filters"},{"location":"ansible/filters/#filters","text":"","title":"Filters"},{"location":"ansible/filters/#use-case","text":"This was developed to perform post-deployment verification against a RESTful service. The tomcat app startup script would always return before the application was ready to serve traffic and also there is a belt-and-braces check against a version endpoint to check that the url is serving the version on the application we believe should have just been deployed. The uri module handles getting what is unfortunately in this case html, then we need to extract the application version from the content. Cue Ansible Filters","title":"Use-case"},{"location":"ansible/filters/#regex_search","text":"regex_search can perform the search and return the matched string. I won't share the whole html here, but note there are multiple versions in the html with no unique IDs on any of the elements. So how to extract just the application version? Although in python we can get this using a single regex and a match group, regex_search cannot return just the match group, only the whole match. < li > application version : 7.12.0-SNAPSHOT </ li > We can do this in two simple steps using regex_search, the first filter matches the project version and sets that as a fact which we can search again and this time there is only one version number in the string to be searched and extracting the version number is now easy with a second regex. - name : extract version element from response set_fact : deployed_application_version_element : \"{{ response | regex_search('application version : ([\\\\w\\\\.\\\\-]+)') }}\" - name : extract version string from element set_fact : deployed_application_version : \"{{ deployed_application_version_element | regex_search('(\\\\d+.*$)') }}\" - name : check facts debug : var : deployed_application_version Note Ansible (2.6) does not support having both facts set in a single task. The following code prodeces the error below because the first fact is not set when the second fact tries to reference it. - name : extract versions from response set_fact : deployed_application_version_element : \"{{ response | regex_search('application version : ([\\\\w\\\\.\\\\-]+)') }}\" deployed_application_version : \"{{ deployed_application_version_element | regex_search('(\\\\d+.*$)') }}\" TASK [ extract versions from response ] ***************************************************************************************** fatal: [ localhost ] : FAILED! = > { \"msg\" : \"Unexpected templating type error occurred on ({{ deployed_application_version_element | regex_search('(\\\\\\\\d+.* $ )') }}): expected string or buffer\" } The error is clearer if we drop the regex_search filter from the second fact, although as always we must read the Ansible error message carefully and fully. - name : extract versions from response set_fact : deployed_application_version_element : \"{{ response | regex_search('application version : ([\\\\w\\\\.\\\\-]+)') }}\" deployed_application_version : \"{{ deployed_application_version_element }}\" ```bash TASK [extract versions from response] ***************************************************************************************** fatal: [localhost]: FAILED! => {\"msg\": \"The task includes an option with an undefined variable. The error was: 'deployed_project_version_match' is undefined\\n\\nThe error appears to have been in '/Users/agar/code/agarthetiger/ansible/check-version.yml': line 9, column 7, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n\\n - name: extract versions from response\\n ^ here\\n\"}","title":"regex_search"},{"location":"ansible/filters/#regex_replace","text":"It would be elegant to get this in one step and we can do that with regex_replace, where the replacement string can reference match groups. We need to modify the original regex slightly so the entire string is matched, in order for just the replacement match group to become the returned string. Lets break the regex down. The leading ^.* and trailing .*?$ ensure that the whole string is matched and replaced by the match group application version : matches the text in the list html element for the version we're interested in ([\\w\\.\\-]) matches any word character (letters, numbers and underscore) plus dot and hyphen and the round brackets around this expression mark it as the first (and only) match group. - name : extract application version from response set_fact : deployed_application_version : \"{{ response | regex_replace('^.*application version : ([\\\\w\\\\.\\\\-]+).*?$', '\\\\1') }}\" Online tools like pythex.org can be useful for quickly testing regular expressions. Danger Be very careful about what you paste into online tools like this. Always triple check that you are never sharing anything sensitive and if in doubt test a rexeg locally, even if it is more cumbersome. Security is never worth risking for speed.","title":"regex_replace"},{"location":"ansible/playbooks/","text":"Playbooks \u00b6 Conditional checks \u00b6 Conditional checks use the when: syntax. The Ansible documentation states \"...When conditions are raw Jinja2 expressions and don't require double quotes for variable interpolation.\", which is not entirely accurate. When conditions can use raw Jinja2 expressions but can execute regular python code so can access methods like String.find() to check for a text match in a String. Multiple conditions should be enclosed with parenthesis, multiple conditions can be specified in a list where they are all required to be true (logical AND). Conditional check examples \u00b6 when : - tomcat_status_result.stdout.find(\"JVM is running.\") > -1 - tomcat_status_result.stderr != \"\" - tomcat_status_result.rc == 0 --- - hosts : all tasks : - name : \"print inventory vars\" debug : var : \"{{ item }}\" with_items : - inventory_dir - inventory_file when : inventory_dir | regex_search('dev$') - hosts : all tasks : - name : \"apply stub role\" include_role : name : issuer-wallet-stub when : inventory_dir | regex_search('dev$') Filters \u00b6 See documentation on filters . Filters use Jinja2, and Ansible ships with some extra ones to those available in Jinja2. Remember that if using a filter in a conditional statement that python methods are also accessible. Also note that the online Jinja2 documentation doesn't go back to python-jinja2 2.7 which is what is provided in RedHat repos for RHEL7. There are no RHEL plans to update Jinja2 to anything later, so what you read on the Jinja website will include features not available to Ansible on RHEL7, because of the Jinja2 version. To select an item from a list, based on an attribute, use the selectattr with the match filter, as the equalsto filter is only available in Jinja2 2.8. Given a yaml file like this, projects : - name : project-a version : '2.3.4-SNAPSHOT' - name : project-b version : '1.2.3-SNAPSHOT' - name : project-c value : '5.6.7-SNAPSHOT' Select the version of project-b using the following expression. - hosts : project-hosts-b vars : - deployable_version : \"{{ projects | selectattr('name', 'match', '^project-b$') | map(attribute='version') | list | first }}\" References jinja2-selectattr-filter Debug \u00b6 Use this to debug what is available in hostvars quickly. gather_facts: false would not be a useful option on first execution of this. - hosts: ondemand gather_facts: false tasks: - name: \"test hostvars for target_environment\" debug: var: hostvars You can print a message with variable information in it. - debug: msg: \"System {{ inventory_hostname }} has gateway {{ ansible_default_ipv4.gateway }} \" when: ansible_default_ipv4.gateway is defined You can dump the contents of a list or map, and set a verbosity level, below which the debug will not output anything. - name: Display all variables/facts known for a host debug: var: hostvars[inventory_hostname] verbosity: 4 See documentation for the debug module . Play options \u00b6 Disable facts gathering - hosts: all gather_facts: false Import and Include \u00b6 Ansible documentation on Creating reusable playbooks Note there are trade-offs when selecting between static and dynamic, any import* tasks will be static, any include* tasks will be dynamic. All import* statements are pre-processed at the time playbooks are parsed. All include* statements are processed as they encountered during the execution of the playbook.","title":"Playbooks"},{"location":"ansible/playbooks/#playbooks","text":"","title":"Playbooks"},{"location":"ansible/playbooks/#conditional-checks","text":"Conditional checks use the when: syntax. The Ansible documentation states \"...When conditions are raw Jinja2 expressions and don't require double quotes for variable interpolation.\", which is not entirely accurate. When conditions can use raw Jinja2 expressions but can execute regular python code so can access methods like String.find() to check for a text match in a String. Multiple conditions should be enclosed with parenthesis, multiple conditions can be specified in a list where they are all required to be true (logical AND).","title":"Conditional checks"},{"location":"ansible/playbooks/#conditional-check-examples","text":"when : - tomcat_status_result.stdout.find(\"JVM is running.\") > -1 - tomcat_status_result.stderr != \"\" - tomcat_status_result.rc == 0 --- - hosts : all tasks : - name : \"print inventory vars\" debug : var : \"{{ item }}\" with_items : - inventory_dir - inventory_file when : inventory_dir | regex_search('dev$') - hosts : all tasks : - name : \"apply stub role\" include_role : name : issuer-wallet-stub when : inventory_dir | regex_search('dev$')","title":"Conditional check examples"},{"location":"ansible/playbooks/#filters","text":"See documentation on filters . Filters use Jinja2, and Ansible ships with some extra ones to those available in Jinja2. Remember that if using a filter in a conditional statement that python methods are also accessible. Also note that the online Jinja2 documentation doesn't go back to python-jinja2 2.7 which is what is provided in RedHat repos for RHEL7. There are no RHEL plans to update Jinja2 to anything later, so what you read on the Jinja website will include features not available to Ansible on RHEL7, because of the Jinja2 version. To select an item from a list, based on an attribute, use the selectattr with the match filter, as the equalsto filter is only available in Jinja2 2.8. Given a yaml file like this, projects : - name : project-a version : '2.3.4-SNAPSHOT' - name : project-b version : '1.2.3-SNAPSHOT' - name : project-c value : '5.6.7-SNAPSHOT' Select the version of project-b using the following expression. - hosts : project-hosts-b vars : - deployable_version : \"{{ projects | selectattr('name', 'match', '^project-b$') | map(attribute='version') | list | first }}\" References jinja2-selectattr-filter","title":"Filters"},{"location":"ansible/playbooks/#debug","text":"Use this to debug what is available in hostvars quickly. gather_facts: false would not be a useful option on first execution of this. - hosts: ondemand gather_facts: false tasks: - name: \"test hostvars for target_environment\" debug: var: hostvars You can print a message with variable information in it. - debug: msg: \"System {{ inventory_hostname }} has gateway {{ ansible_default_ipv4.gateway }} \" when: ansible_default_ipv4.gateway is defined You can dump the contents of a list or map, and set a verbosity level, below which the debug will not output anything. - name: Display all variables/facts known for a host debug: var: hostvars[inventory_hostname] verbosity: 4 See documentation for the debug module .","title":"Debug"},{"location":"ansible/playbooks/#play-options","text":"Disable facts gathering - hosts: all gather_facts: false","title":"Play options"},{"location":"ansible/playbooks/#import-and-include","text":"Ansible documentation on Creating reusable playbooks Note there are trade-offs when selecting between static and dynamic, any import* tasks will be static, any include* tasks will be dynamic. All import* statements are pre-processed at the time playbooks are parsed. All include* statements are processed as they encountered during the execution of the playbook.","title":"Import and Include"},{"location":"bash/reference/","text":"Bash reference \u00b6 Process substitution \u00b6 This example was used to upload a file to post a file to a site requiriing authentication, without ever writing the credentials to disk or having them exposed in the command history or the running process list while the command is executing. Note that this code was run from a Jenkinsfile pipeline shared library, so the ${} variables are coming from Jenkins (Groovy) DSL. response_code = \\$ ( curl -w % { http_code } --netrc-file < ( cat <<< 'machine $uploadDomain login ${Username} password ${Password}' ) -sS --upload-file ${ filename } '${uploadUrl}' ) Breaking this command down <<<'Something here' is a here string , a variant of a here doc where the (variables in the) string are expanded before being fed to standard input. <( command ) is process substitution where the standard output of one process can be fed to the standard input of another process. In this case we're using process substitution and cat to feed curl with a netrc 'file' without ever writing the file to disk. --netrc-file is a curl option which can be used to provide credentials for curl to present when connecting to specified domains. Comments \u00b6 The # symbol is a bash comment, and can also be used to to stop processing of command line options. This is useful in git alias commands where we want to reference command line args in the git alias. Without the hash at the end of the alias below, using GitBash on Windows 10, the code will error with a message like grep : develop : No such file or directory . Adding the comment halts processing and the command works as expected. alias.prune-merged = !git branch --merged \" $1 \" | grep -v -e develop -e master -e \" $1 \" # Explain Shell \u00b6 Explain Shell can be useful to start to understand complex bash commands. Warning Remember this is a public (untrusted) website so be very careful about what you paste in here.","title":"Bash reference"},{"location":"bash/reference/#bash-reference","text":"","title":"Bash reference"},{"location":"bash/reference/#process-substitution","text":"This example was used to upload a file to post a file to a site requiriing authentication, without ever writing the credentials to disk or having them exposed in the command history or the running process list while the command is executing. Note that this code was run from a Jenkinsfile pipeline shared library, so the ${} variables are coming from Jenkins (Groovy) DSL. response_code = \\$ ( curl -w % { http_code } --netrc-file < ( cat <<< 'machine $uploadDomain login ${Username} password ${Password}' ) -sS --upload-file ${ filename } '${uploadUrl}' ) Breaking this command down <<<'Something here' is a here string , a variant of a here doc where the (variables in the) string are expanded before being fed to standard input. <( command ) is process substitution where the standard output of one process can be fed to the standard input of another process. In this case we're using process substitution and cat to feed curl with a netrc 'file' without ever writing the file to disk. --netrc-file is a curl option which can be used to provide credentials for curl to present when connecting to specified domains.","title":"Process substitution"},{"location":"bash/reference/#comments","text":"The # symbol is a bash comment, and can also be used to to stop processing of command line options. This is useful in git alias commands where we want to reference command line args in the git alias. Without the hash at the end of the alias below, using GitBash on Windows 10, the code will error with a message like grep : develop : No such file or directory . Adding the comment halts processing and the command works as expected. alias.prune-merged = !git branch --merged \" $1 \" | grep -v -e develop -e master -e \" $1 \" #","title":"Comments"},{"location":"bash/reference/#explain-shell","text":"Explain Shell can be useful to start to understand complex bash commands. Warning Remember this is a public (untrusted) website so be very careful about what you paste in here.","title":"Explain Shell"},{"location":"cheat-sheets/jenkins/","text":"Jenkins \u00b6 Jenkins Master urls \u00b6 Replace localhost:8080 with the address of your Jenkins instance. Pipeline Syntax Snippet Generator, almost always your friend :) - http://localhost:8080/pipeline-syntax/ GDSL - http://localhost:8080/pipeline-syntax/gdsl Global 'variables' docs - http://localhost:8080/pipeline-syntax/globals In a shared or global library, add a .txt file alongside a .groovy file with text or HTML to automatically have add help to this globals page. External References \u00b6 Java doc for Jenkins API - https://javadoc.jenkins-ci.org/","title":"Jenkins"},{"location":"cheat-sheets/jenkins/#jenkins","text":"","title":"Jenkins"},{"location":"cheat-sheets/jenkins/#jenkins-master-urls","text":"Replace localhost:8080 with the address of your Jenkins instance. Pipeline Syntax Snippet Generator, almost always your friend :) - http://localhost:8080/pipeline-syntax/ GDSL - http://localhost:8080/pipeline-syntax/gdsl Global 'variables' docs - http://localhost:8080/pipeline-syntax/globals In a shared or global library, add a .txt file alongside a .groovy file with text or HTML to automatically have add help to this globals page.","title":"Jenkins Master urls"},{"location":"cheat-sheets/jenkins/#external-references","text":"Java doc for Jenkins API - https://javadoc.jenkins-ci.org/","title":"External References"},{"location":"jenkins/JenkinsPlugins/","text":"Jenkins Plugins \u00b6 Anyone who has ever administered Jenkins will know that plugins are both essential and problematic. Jenkins can be enhanced through an extensive collection of community maintained plugins, and therein lies the problem. \"Want to do 'A' with Jenkins? Yes you can. Want to do 'B' with Jenkins? Yes you can.\", goes the sales pitch. However, often A and B are implemented in community plugins which may conflict when installed 1 or may not work together even if both are installed. Custom tool plugin \u00b6 Not compatible with Pipelines until JENKINS-30680 is resolved. This has been open since September 2015. \"Installed\" must include restarting the Jenkins Master. I've had a plugin cause a conflict when installed, so I uninstalled it. The uninstall didn't resolve the conflict so I restarted to completely remove it. 12 hours later at 11:30pm I finally got the Jenkins Master up and running again. I'd been tracking down all the recent changes working backwards to find that a plugin installed 8 months previously (before I joined the company of course) was causing the conflict. I'd overlooked the signs initially, not believing that a change made 8 months ago could have caused the problem. The master had not been restarted in 8 months. If the master had been restarted at the time, the problem would have been detected immediately and the change rolled back right away. Lesson learned, always restart Jenkins after installing or removing plugins and check the logs to ensure the instance comes up cleanly. \u21a9","title":"Jenkins Plugins"},{"location":"jenkins/JenkinsPlugins/#jenkins-plugins","text":"Anyone who has ever administered Jenkins will know that plugins are both essential and problematic. Jenkins can be enhanced through an extensive collection of community maintained plugins, and therein lies the problem. \"Want to do 'A' with Jenkins? Yes you can. Want to do 'B' with Jenkins? Yes you can.\", goes the sales pitch. However, often A and B are implemented in community plugins which may conflict when installed 1 or may not work together even if both are installed.","title":"Jenkins Plugins"},{"location":"jenkins/JenkinsPlugins/#custom-tool-plugin","text":"Not compatible with Pipelines until JENKINS-30680 is resolved. This has been open since September 2015. \"Installed\" must include restarting the Jenkins Master. I've had a plugin cause a conflict when installed, so I uninstalled it. The uninstall didn't resolve the conflict so I restarted to completely remove it. 12 hours later at 11:30pm I finally got the Jenkins Master up and running again. I'd been tracking down all the recent changes working backwards to find that a plugin installed 8 months previously (before I joined the company of course) was causing the conflict. I'd overlooked the signs initially, not believing that a change made 8 months ago could have caused the problem. The master had not been restarted in 8 months. If the master had been restarted at the time, the problem would have been detected immediately and the change rolled back right away. Lesson learned, always restart Jenkins after installing or removing plugins and check the logs to ensure the instance comes up cleanly. \u21a9","title":"Custom tool plugin"},{"location":"jenkins/PipelineDSL/","text":"Pipeline DSL Notes \u00b6 Groovy closures \u00b6 There has been a long-standing bug with using closures like .each {} on collections, where the behaviour was incorrect and inconsistent. This gist demonstrates some of this behaviour. The bug has now been fixed in plugin:workflow-cps 2.33 although it's worth noting that there may still be other bugs like this which at the time of writing are still unresolved. Script approvals from libraries \u00b6 If you are a non-admin user writing Jenkins shared libraries, note that non-whitelisted methods will be blocked with an error indicating a script approval is required. Unfortunately library code does not raise a script approval for an admin to approve and the code in question has to be added to a Pipeline job where the methods are called directly from a Pipeline job using \"Pipeline script\" or \"Pipeline Script from SCM\". To address this, a pattern I have adopted is to include a Groovy Jenkinsfile in the shared library repository called script-approvals.groovy which makes all the method calls requiring approval. It is then easy to create a Jenkins job which executes this pipeline, which can be repeatedly executed by the team responsible for approving scripts until the script runs to completion. This is useful in environments where there are new or multiple Jenkins masters which a library will be used on.","title":"Pipeline DSL Notes"},{"location":"jenkins/PipelineDSL/#pipeline-dsl-notes","text":"","title":"Pipeline DSL Notes"},{"location":"jenkins/PipelineDSL/#groovy-closures","text":"There has been a long-standing bug with using closures like .each {} on collections, where the behaviour was incorrect and inconsistent. This gist demonstrates some of this behaviour. The bug has now been fixed in plugin:workflow-cps 2.33 although it's worth noting that there may still be other bugs like this which at the time of writing are still unresolved.","title":"Groovy closures"},{"location":"jenkins/PipelineDSL/#script-approvals-from-libraries","text":"If you are a non-admin user writing Jenkins shared libraries, note that non-whitelisted methods will be blocked with an error indicating a script approval is required. Unfortunately library code does not raise a script approval for an admin to approve and the code in question has to be added to a Pipeline job where the methods are called directly from a Pipeline job using \"Pipeline script\" or \"Pipeline Script from SCM\". To address this, a pattern I have adopted is to include a Groovy Jenkinsfile in the shared library repository called script-approvals.groovy which makes all the method calls requiring approval. It is then easy to create a Jenkins job which executes this pipeline, which can be repeatedly executed by the team responsible for approving scripts until the script runs to completion. This is useful in environments where there are new or multiple Jenkins masters which a library will be used on.","title":"Script approvals from libraries"},{"location":"jenkins/PipelineSteps/","text":"Pipeline Steps \u00b6 sh \u00b6 The bash defaults for running sh script is set -xe which will print all commands and arguments when executed (-x) and terminate on any non-zero exit code (-e). Note that this behaviour changes if using returnStatus : true . In this case the sh step will not terminate on error but the build will still be marked as failed overall, but the build execution will not be halted. It is down to you to check the value returned by the sh step. Quotes and sh \u00b6 Quotes in Jenkinsfiles are not always as simple to get right as you may think. Jenkinsfiles are groovy DSL, which often reference environment variables such as the values for parameterised jobs. Environment variables and groovy vars may both need to be passed to an echo step or to a sh step, sometimes to be evaluated in the Jenkins context, sometimes by the sh. An apparently simple task using process substitution to create a virtual .netrc file or creating a temporary json file (without using writeJSON ) is often not quite so straightforward unless you remember the rules for quotations for each context. Debugging sh scripts \u00b6 Add 'cat -n \\$0' into sh scripts to get Jenkins to echo out the shell script and line numbers into the console log, to check what Jenkins is actually executing on the agent. From this blog NonCPS annotation \u00b6 DSL code can only call methods or use classes which are serializable, unless the non-serializable methods are annotated with @NonCPS. Note there are additional considerations to be aware of. Methods annotated with @NonCPS cannot call any DSL methods, or any other methods unless the other methods are also annotated as NonCPS. Methods annotated with @NonCPS which call other methods will have undefined behaviour, including but not limited to returning unexpected values. Echo appears to be safe to use to log information from @NonCPS methods.","title":"Pipeline Steps"},{"location":"jenkins/PipelineSteps/#pipeline-steps","text":"","title":"Pipeline Steps"},{"location":"jenkins/PipelineSteps/#sh","text":"The bash defaults for running sh script is set -xe which will print all commands and arguments when executed (-x) and terminate on any non-zero exit code (-e). Note that this behaviour changes if using returnStatus : true . In this case the sh step will not terminate on error but the build will still be marked as failed overall, but the build execution will not be halted. It is down to you to check the value returned by the sh step.","title":"sh"},{"location":"jenkins/PipelineSteps/#quotes-and-sh","text":"Quotes in Jenkinsfiles are not always as simple to get right as you may think. Jenkinsfiles are groovy DSL, which often reference environment variables such as the values for parameterised jobs. Environment variables and groovy vars may both need to be passed to an echo step or to a sh step, sometimes to be evaluated in the Jenkins context, sometimes by the sh. An apparently simple task using process substitution to create a virtual .netrc file or creating a temporary json file (without using writeJSON ) is often not quite so straightforward unless you remember the rules for quotations for each context.","title":"Quotes and sh"},{"location":"jenkins/PipelineSteps/#debugging-sh-scripts","text":"Add 'cat -n \\$0' into sh scripts to get Jenkins to echo out the shell script and line numbers into the console log, to check what Jenkins is actually executing on the agent. From this blog","title":"Debugging sh scripts"},{"location":"jenkins/PipelineSteps/#noncps-annotation","text":"DSL code can only call methods or use classes which are serializable, unless the non-serializable methods are annotated with @NonCPS. Note there are additional considerations to be aware of. Methods annotated with @NonCPS cannot call any DSL methods, or any other methods unless the other methods are also annotated as NonCPS. Methods annotated with @NonCPS which call other methods will have undefined behaviour, including but not limited to returning unexpected values. Echo appears to be safe to use to log information from @NonCPS methods.","title":"NonCPS annotation"},{"location":"jenkins/administration/","text":"Jenkins Administration \u00b6 Security \u00b6 LTS does not mean patched \u00b6 Be aware that the Jenkins LTS release ships with default plugins which are not removable from the Jenkins instance and can be pinned to a low version, missing security updates. One such example is Jenkins LTS v2.176.2 which bundles the Ant plugin at version 1.2, released in 2013. There was a security advisory on 22 nd Jan 2018 which identified an XSS vulnerability in this plugin affecting versions up to and including 1.7 . At the time of writing this a year and a half later, the current LTS Jenkins is still bundling a version of this plugin from nearly 5 years prior. Many users will have no need for this plugin to be present, but it is not removable. I have to presume that the level of automated testing for Jenkins/CloudBees to perform even with less frequent LTS releases is prohibitive from patching all plugins even for security fixes.","title":"Jenkins Administration"},{"location":"jenkins/administration/#jenkins-administration","text":"","title":"Jenkins Administration"},{"location":"jenkins/administration/#security","text":"","title":"Security"},{"location":"jenkins/administration/#lts-does-not-mean-patched","text":"Be aware that the Jenkins LTS release ships with default plugins which are not removable from the Jenkins instance and can be pinned to a low version, missing security updates. One such example is Jenkins LTS v2.176.2 which bundles the Ant plugin at version 1.2, released in 2013. There was a security advisory on 22 nd Jan 2018 which identified an XSS vulnerability in this plugin affecting versions up to and including 1.7 . At the time of writing this a year and a half later, the current LTS Jenkins is still bundling a version of this plugin from nearly 5 years prior. Many users will have no need for this plugin to be present, but it is not removable. I have to presume that the level of automated testing for Jenkins/CloudBees to perform even with less frequent LTS releases is prohibitive from patching all plugins even for security fixes.","title":"LTS does not mean patched"},{"location":"python/mkdocs/","text":"MKDocs \u00b6 This site is built with MKDocs, hosted on GitHub Pages and automatically published on commit to master from Travis CI. Good documentation is vital for any enterprise IT team to publish shared practices or software components. To empower other teams and allow them to easily consume the output is key, in order to alleviate the publishing team from becoming yet-another-enterprise-bottleneck. MKDocs and GitHub Pages allows teams to publish documentation without having to worry about site hosting, and the documentation can be easily written in markdown. CI deployments to GitHub Pages with Travis-CI \u00b6 To publish from master onto the site served from the gh-pages branch I've used Travis CI for this open source site. I based my .travis.yml file on Derek Weitzel's blog post . As this was my first time using Travis I didn't initally understand what every line was for. A little piece of me dies whenever I see people cutting and pasting from the internet and executing code, or worse just \"curl bash piping\", especially in an enterprise organisation when they have no idea what code they are running. I've seen that happen way more times than I'd like. Anyway, here is my breakdown of the original script, followed by my enhancements. env : global : - GIT_NAME : \"'Markdown autodeploy'\" - GIT_EMAIL : djw8605 @ gmail . com - GH_REF : git @ github . com : opensciencegrid / security . git language : python Set some environment variables to use later in the job. Setting the language to python tells Travis CI to run this job on a docker container with python and pip pre-installed. before_script: - pip install mkdocs - pip install MarkdownHighlight This before_script phase will run prior to the (build) script section running. In the Job lifecycle Travis CI provides an install phase for installing dependencies which should be used instead. With this script there may be no practical difference between running these steps under the install or before_script phases, however I believe following the patterns intended by the tools is often an easier path to travel in the long term. script: - openssl aes-256-cbc -K $encrypted_1d262b48bc9b_key -iv $encrypted_1d262b48bc9b_iv -in deploy-key.enc -out deploy-key -d - chmod 600 deploy-key - eval `ssh-agent -s` - ssh-add deploy-key MKDocs needs to be able to commit and push to the gh-pages branch in the git repository. The openssl command decrypts the private ssh deploy key, then we limit the permissions and add the key to the running ssh agent. Restricting permissions is required otherwise ssh-add will refuse to add the overly permissive private key file. The eval statement is required to set the apropriate environment variables for the current shell instance so that you can connect to it with ssh-add. - git config user.name \"Automatic Publish\" - git config user.email \"djw8605@gmail.com\" - git remote add gh-token \" ${ GH_REF } \"; - git fetch gh-token && git fetch gh-token gh-pages:gh-pages; These steps are required in order for mkdocs to push to the GitHub gh-pages branch. In order to understand why, we need to look at the build log which shows Travis CI running something like the following $ git clone --depth = 50 --branch = master https://github.com/agarthetiger/mkdocs.git agarthetiger/mkdocs Cloning into 'agarthetiger/mkdocs' ... $ cd agarthetiger/mkdocs $ git checkout -qf cffa8d732d907f62a08262db229ee5899e2fc795 There would be two issues with trying to run mkdocs gh-deploy without the bulk of code in this script phase. First off, the --depth option on git clone implies --single-branch . If you add git branch -r or git remote show origin as steps into the script you can see that there is apparently no gh-pages branch in the remote. This behaviour can be changed by specifying git.depth: false in .travis.yml which will cause Travis CI to drop the --depth option and the clone will then fetch all branches. This can also be changed by telling git to clone all branches but there is still one more issue. The other issue is that we haven't provided a GitHub token or credentials to authenticate to the https GitHub remote with, so even if we add the branch to the origin we still can't push. The clone operation works as this is a public GitHub repository. Using a deploy key and ssh is preferable because any exposure of the credentials in this case would only compromise one repository via the deploy key, rather than compromising all repositories if using a GitHub token. Back to the original script, and adding a 2 nd remote allows the additional branches to be cloned over ssh and also to be pushed to using the decrypted key added to the ssh agent earlier for authentication. The fetch command can be simplified from the example because the initial git fetch gh-token fetches all branches. - if [ \" ${ TRAVIS_PULL_REQUEST } \" = \"false\" ]; then echo \"Pushing to github\"; PYTHONPATH=src/ mkdocs gh-deploy -v --clean --remote-name gh-token; git push gh-token gh-pages; fi; This line first checks to see whether the build was triggered from a Pull Request. Technically this could be avoided, because what we're doing here is building and deploying the website. We could build the site in the script phase of the .travis.yml file and then have all this ssh, git and deploy code running from the deploy phase. This is because the deploy phase is always skipped for Pull Requests . The script provider for deployments requires the script be defined in an external file, which is executed with bash. I've refactored this in my version, as the script example from the blog post is essentially individual bash commands in a yaml dictionary it was quick to translate into a file. Adding set -e to the file also allows the script to fail fast and terminate on the first non-zero exit code from a command. Failing fast on error is possibly but ugly to accomplish in the .travis.yml file, requiring every line to append || travis_terminate 1 and even this approach has problems . Travis CI Enhancements \u00b6 My yaml file looks like this. language : python branches : only : - master git : depth : 1 install : - pip install mkdocs - pip install mkdocs - material script : - if [[ $TRAVIS_PULL_REQUEST == \"false\" ]]; then mkdocs build -- strict ; fi ; deploy : skip_cleanup : true provider : script script : bash travis - ci / deploy - to - gh - pages . sh on : branch : master I kept the --depth option to still shallow clone the repo with a single branch for the build. Using the branches whitelist I could remove the requirement to check the branch name later in the script, and using a deploy phase meant no additional logic is required to check for pull requests or the branch in the shell script travis-ci/deploy-to-gh-pages.sh. deploy.skip_cleanup stops Travis from stashing changes in the workspace between the (build) script and deploy phases.","title":"MKDocs"},{"location":"python/mkdocs/#mkdocs","text":"This site is built with MKDocs, hosted on GitHub Pages and automatically published on commit to master from Travis CI. Good documentation is vital for any enterprise IT team to publish shared practices or software components. To empower other teams and allow them to easily consume the output is key, in order to alleviate the publishing team from becoming yet-another-enterprise-bottleneck. MKDocs and GitHub Pages allows teams to publish documentation without having to worry about site hosting, and the documentation can be easily written in markdown.","title":"MKDocs"},{"location":"python/mkdocs/#ci-deployments-to-github-pages-with-travis-ci","text":"To publish from master onto the site served from the gh-pages branch I've used Travis CI for this open source site. I based my .travis.yml file on Derek Weitzel's blog post . As this was my first time using Travis I didn't initally understand what every line was for. A little piece of me dies whenever I see people cutting and pasting from the internet and executing code, or worse just \"curl bash piping\", especially in an enterprise organisation when they have no idea what code they are running. I've seen that happen way more times than I'd like. Anyway, here is my breakdown of the original script, followed by my enhancements. env : global : - GIT_NAME : \"'Markdown autodeploy'\" - GIT_EMAIL : djw8605 @ gmail . com - GH_REF : git @ github . com : opensciencegrid / security . git language : python Set some environment variables to use later in the job. Setting the language to python tells Travis CI to run this job on a docker container with python and pip pre-installed. before_script: - pip install mkdocs - pip install MarkdownHighlight This before_script phase will run prior to the (build) script section running. In the Job lifecycle Travis CI provides an install phase for installing dependencies which should be used instead. With this script there may be no practical difference between running these steps under the install or before_script phases, however I believe following the patterns intended by the tools is often an easier path to travel in the long term. script: - openssl aes-256-cbc -K $encrypted_1d262b48bc9b_key -iv $encrypted_1d262b48bc9b_iv -in deploy-key.enc -out deploy-key -d - chmod 600 deploy-key - eval `ssh-agent -s` - ssh-add deploy-key MKDocs needs to be able to commit and push to the gh-pages branch in the git repository. The openssl command decrypts the private ssh deploy key, then we limit the permissions and add the key to the running ssh agent. Restricting permissions is required otherwise ssh-add will refuse to add the overly permissive private key file. The eval statement is required to set the apropriate environment variables for the current shell instance so that you can connect to it with ssh-add. - git config user.name \"Automatic Publish\" - git config user.email \"djw8605@gmail.com\" - git remote add gh-token \" ${ GH_REF } \"; - git fetch gh-token && git fetch gh-token gh-pages:gh-pages; These steps are required in order for mkdocs to push to the GitHub gh-pages branch. In order to understand why, we need to look at the build log which shows Travis CI running something like the following $ git clone --depth = 50 --branch = master https://github.com/agarthetiger/mkdocs.git agarthetiger/mkdocs Cloning into 'agarthetiger/mkdocs' ... $ cd agarthetiger/mkdocs $ git checkout -qf cffa8d732d907f62a08262db229ee5899e2fc795 There would be two issues with trying to run mkdocs gh-deploy without the bulk of code in this script phase. First off, the --depth option on git clone implies --single-branch . If you add git branch -r or git remote show origin as steps into the script you can see that there is apparently no gh-pages branch in the remote. This behaviour can be changed by specifying git.depth: false in .travis.yml which will cause Travis CI to drop the --depth option and the clone will then fetch all branches. This can also be changed by telling git to clone all branches but there is still one more issue. The other issue is that we haven't provided a GitHub token or credentials to authenticate to the https GitHub remote with, so even if we add the branch to the origin we still can't push. The clone operation works as this is a public GitHub repository. Using a deploy key and ssh is preferable because any exposure of the credentials in this case would only compromise one repository via the deploy key, rather than compromising all repositories if using a GitHub token. Back to the original script, and adding a 2 nd remote allows the additional branches to be cloned over ssh and also to be pushed to using the decrypted key added to the ssh agent earlier for authentication. The fetch command can be simplified from the example because the initial git fetch gh-token fetches all branches. - if [ \" ${ TRAVIS_PULL_REQUEST } \" = \"false\" ]; then echo \"Pushing to github\"; PYTHONPATH=src/ mkdocs gh-deploy -v --clean --remote-name gh-token; git push gh-token gh-pages; fi; This line first checks to see whether the build was triggered from a Pull Request. Technically this could be avoided, because what we're doing here is building and deploying the website. We could build the site in the script phase of the .travis.yml file and then have all this ssh, git and deploy code running from the deploy phase. This is because the deploy phase is always skipped for Pull Requests . The script provider for deployments requires the script be defined in an external file, which is executed with bash. I've refactored this in my version, as the script example from the blog post is essentially individual bash commands in a yaml dictionary it was quick to translate into a file. Adding set -e to the file also allows the script to fail fast and terminate on the first non-zero exit code from a command. Failing fast on error is possibly but ugly to accomplish in the .travis.yml file, requiring every line to append || travis_terminate 1 and even this approach has problems .","title":"CI deployments to GitHub Pages with Travis-CI"},{"location":"python/mkdocs/#travis-ci-enhancements","text":"My yaml file looks like this. language : python branches : only : - master git : depth : 1 install : - pip install mkdocs - pip install mkdocs - material script : - if [[ $TRAVIS_PULL_REQUEST == \"false\" ]]; then mkdocs build -- strict ; fi ; deploy : skip_cleanup : true provider : script script : bash travis - ci / deploy - to - gh - pages . sh on : branch : master I kept the --depth option to still shallow clone the repo with a single branch for the build. Using the branches whitelist I could remove the requirement to check the branch name later in the script, and using a deploy phase meant no additional logic is required to check for pull requests or the branch in the shell script travis-ci/deploy-to-gh-pages.sh. deploy.skip_cleanup stops Travis from stashing changes in the workspace between the (build) script and deploy phases.","title":"Travis CI Enhancements"}]}